
"""

A human-in-the-loop workflow integrates human input into automated processes, allowing for decisions, validation, or corrections at key stages.

This is especially useful in LLM-based applications, where the underlying model may generate occasional inaccuracies.

---> Use-cases:

1. Reviewing tool calls: Human can review, edit, or approve tool calls requested by the LLM before tool execution.
2. Validating LLM outputs: Humans can review, edit, or approve content generated by the LLM
3. Providing context: Enable the LLM to explicitly request human input for clarification or additional details or to support multi-turn conversations.


There are typically three different actions that you can do with a human-in-the-loop workflow:

1. Approve or Reject: 
-----> Pause the graph before a critical step, such as an API call, to review and approve the action.

If the action is rejected, you can prevent the graph from executing the step, and potentially take an alternative action. This pattern often involve routing the graph based on the human's input.

2. Review & Edit State:

-----> A human can review and edit the state of the graph. This is usefulfor correcting mistakes or updating the state with additional information.

3. Review Tool Calls:
A human can review and edit the output form the LLM before proceeding.

This is particularlly critical in applications wwhere the tool calls requested by the LLM may be sensitive or require human oversight.
"""



from typing import TypedDict, Annotated
from langchain_core.messages import HumanMessage
from langgraph.graph import add_messages, StateGraph, END
from langchain_groq import ChatGroq

from dotenv import load_dotenv
load_dotenv()

class State(TypedDict): 
    messages: Annotated[list, add_messages]

llm = ChatGroq(model="llama-3.3-70b-versatile")

GENERATE_POST = "generate_post"
GET_REVIEW_DECISION = "get_review_decision"
POST = "post"
COLLECT_FEEDBACK = "collect_feedback"

def generate_post(state: State): 
    return {
        "messages": [llm.invoke(state["messages"])]
    }

def get_review_decision(state: State):  
    post_content = state["messages"][-1].content 
    
    print("\nðŸ“¢ Current LinkedIn Post:\n")
    print(post_content)
    print("\n")

    decision = input("Post to LinkedIn? (yes/no): ")

    if decision.lower() == "yes":
        return POST
    else:
        return COLLECT_FEEDBACK


def post(state: State):  
    final_post = state["messages"][-1].content  
    print("\nðŸ“¢ Final LinkedIn Post:\n")
    print(final_post)
    print("\nâœ… Post has been approved and is now live on LinkedIn!")

def collect_feedback(state: State):  
    feedback = input("How can I improve this post?")
    return {
        "messages": [HumanMessage(content=feedback)]
    }

graph = StateGraph(State)

graph.add_node(GENERATE_POST, generate_post)
graph.add_node(GET_REVIEW_DECISION, get_review_decision)
graph.add_node(COLLECT_FEEDBACK, collect_feedback)
graph.add_node(POST, post)

graph.set_entry_point(GENERATE_POST)

graph.add_conditional_edges(GENERATE_POST, get_review_decision)
graph.add_edge(POST, END)
graph.add_edge(COLLECT_FEEDBACK, GENERATE_POST)

app = graph.compile()

response = app.invoke({
    "messages": [HumanMessage(content="Write me a LinkedIn post on AI Agents taking over content creation")]
})

print(response)



"""
Drawbacks of input():

1. Blocking: The program will pause and wait for user input, which can make it difficult to build asynchronous workflows.
2. Not Reusable: The input() function is not reusable in a modular way.
3. Only works in terminals - useless for web pages
4. if your program crashes, all progress is lost.
5. Can only handle one user at a time.
6. lives only in your terminal session.

This is why we use a special method that Langgraph provides called "interrupt"

What is interrupt and Why use it?

1. Special LangGraph function that pauses your workflows nicely
2. Saves your program's state so it can continue later.
3. Works in web apps, APIs , and other interfaces
4. Handles multiple users/sessions at once
5. Survives program crashes and restarts
6. Lets humans take their time to respond
7. Required for any serious human-in-the-loop-system


In **LangGraph**, an **interrupt** is a mechanism that allows the agentâ€™s execution to **pause** at a specific step, **wait for human input**, and then **resume** from the same point.

This is mainly used to build **human-in-the-loop (HITL)** workflows, where humans guide, verify, or approve agent actions during execution.

---

## **2. Why Interrupts Are Needed**

Normally, LangGraph executes the graph **automatically** â€” once it starts, it keeps running until it reaches a terminal node.

But in real-world scenarios, we often need **human intervention**:

* âœ… To **confirm** critical actions (e.g., booking flights, deleting files)
* âœ… To **provide missing inputs** (e.g., when AI doesn't know some info)
* âœ… To **approve or reject** the agentâ€™s decisions before proceeding

Thatâ€™s where **interrupts** come in.

---

## **3. How Interrupts Work**

Hereâ€™s the **lifecycle** of an interrupt in LangGraph:

1. The agent reaches a node where **human input** is required.
2. You call `interrupt()` â†’ **execution pauses**.
3. LangGraph returns control to the **caller** (your app, API, or UI).
4. The human provides input.
5. The graph **resumes from where it stopped**, using the human-provided data.

---

## **4. Using Interrupts in LangGraph**

### **Step 1. Import and Create a Graph**

```python
from langgraph.graph import StateGraph, END
from langgraph.types import interrupt
from typing import TypedDict

class AgentState(TypedDict):
    question: str
    human_feedback: str
```

---

### **Step 2. Define a Node that Interrupts**

```python
def ask_human(state: AgentState):
    # Pause the graph and wait for human input
    feedback = interrupt(
        "Please provide your feedback or decision for the AI"
    )
    # The graph will resume here after the human responds
    return {"human_feedback": feedback}
```

---

### **Step 3. Define the Next Node**

```python
def process_feedback(state: AgentState):
    return {
        "question": f"Thanks for your feedback: {state['human_feedback']}"
    }
```

---

### **Step 4. Build the Graph**

```python
workflow = StateGraph(AgentState)
workflow.add_node("ask_human", ask_human)
workflow.add_node("process_feedback", process_feedback)

workflow.set_entry_point("ask_human")
workflow.add_edge("ask_human", "process_feedback")
workflow.add_edge("process_feedback", END)

app = workflow.compile()
```

---

### **Step 5. Execute with Human-in-the-Loop**

```python
# Step 1 â†’ Start execution
config = {"configurable": {"thread_id": "1"}}
result = app.invoke({"question": "Should I proceed?"}, config=config)

print("Graph paused, waiting for human input:", result)
# At this point, you can collect input from a human (e.g., via UI or CLI)

# Step 2 â†’ Resume execution
final = app.resume("Yes, go ahead!", config=config)

print("Final result:", final)
```

---

## **5. How This Enables Human-in-the-Loop**

* **Before** taking action, the agent **interrupts**.
* **Control returns** to you â€” meaning you can:

  * Show the pending decision to a **human operator**
  * Collect feedback/approval
  * **Resume execution** once the human responds
* This way, **humans stay in control** without breaking the LangGraph flow.

---

## **6. Real-World Example: Booking a Flight**

Imagine building a **travel assistant** using LangGraph:

* The agent finds a **flight ticket**.
* Before booking, you add an **interrupt**:

  ```python
  interrupt(f"Do you want to book this flight for â‚¹5000?")
  ```
* Your app shows a **confirmation popup**.
* If the user clicks **"Yes"**, the graph **resumes** and books the ticket.
* If **"No"**, you redirect or modify the flow.

---

## **7. Summary Table**

| **Feature** | **Without Interrupt** | **With Interrupt (HITL)**          |
| ----------- | --------------------- | ---------------------------------- |
| Control     | Fully autonomous      | Human approval needed              |
| Safety      | Low                   | High                               |
| Flexibility | Fixed workflow        | Dynamic based on human input       |
| Use cases   | Simple automation     | Approvals, feedback, confirmations |

---

## **8. Key Points**

* `interrupt()` **pauses** the LangGraph execution.
* It allows **human-in-the-loop** workflows.
* Works well for:

  * Tool result approvals
  * Agent decision confirmations
  * Missing information requests
* Use `.resume()` to **continue execution** after human input.


In **LangGraph**, there are **two main ways** to use **interrupts** for building **human-in-the-loop (HITL)** workflows:

1. **Using `interrupt()` inside a node function** â†’ the **function-based approach** âœ…
2. **Using the `Interrupt` command class in compiled graphs** â†’ the **command-based approach** ðŸ§©

Both achieve the same goal â€” **pausing execution** and **resuming after human input** â€” but theyâ€™re used differently.

Letâ€™s break them down **step by step** with examples.

---

## **1. Using `interrupt()` Inside a Node Function (Simpler Approach)** âœ…

This is the most **direct** and **common** way to introduce human-in-the-loop behavior.
You simply **call `interrupt()`** inside a node, and LangGraph pauses execution until you **resume** with human input.

---

### **Example**

```python
from langgraph.graph import StateGraph, END
from langgraph.types import interrupt
from typing import TypedDict

# Define state structure
class AgentState(TypedDict):
    question: str
    human_feedback: str

# Node where we pause execution and ask the human
def ask_human(state: AgentState):
    feedback = interrupt("Please provide your feedback")
    return {"human_feedback": feedback}

# Next node after human responds
def process_feedback(state: AgentState):
    return {"question": f"Thanks! Your feedback was: {state['human_feedback']}"}

# Build the graph
workflow = StateGraph(AgentState)
workflow.add_node("ask_human", ask_human)
workflow.add_node("process_feedback", process_feedback)

workflow.set_entry_point("ask_human")
workflow.add_edge("ask_human", "process_feedback")
workflow.add_edge("process_feedback", END)

app = workflow.compile()
```

---

### **Running the Graph**

```python
config = {"configurable": {"thread_id": "123"}}

# First invoke â†’ execution pauses at interrupt()
result = app.invoke({"question": "Should I proceed?"}, config=config)
print(result)  
# Output: Graph paused, waiting for human input

# Later, resume after human provides input
final = app.resume("Yes, go ahead!", config=config)
print(final)
# Output: {'question': 'Thanks! Your feedback was: Yes, go ahead!', 'human_feedback': 'Yes, go ahead!'}
```

---

### **How It Works**

* **Step 1:** The graph starts at `"ask_human"`.
* **Step 2:** `interrupt()` pauses execution.
* **Step 3:** Control returns to the caller (your app/UI).
* **Step 4:** After you collect human input, call `.resume()`.
* **Step 5:** Graph continues from where it stopped.

âœ… **When to use:**

* Simple HITL approvals
* Collecting feedback
* Waiting for user input

---

## **2. Using the `Interrupt` Command Class (Advanced Approach)** ðŸ§©

This is a **lower-level**, more **explicit** way to handle interrupts.
Instead of calling `interrupt()` directly, you **return an `Interrupt` command** from your node.

This approach is useful when you:

* Want **more control** over pause behavior.
* Work with **compiled graphs** where you want to define **interrupts declaratively**.
* Build reusable **command-based workflows**.

---

### **Example**

```python
from langgraph.graph import StateGraph, END
from langgraph.types import Command, Interrupt
from typing import TypedDict

# Define state structure
class AgentState(TypedDict):
    question: str
    human_feedback: str

# Node returns an Interrupt Command
def ask_human(state: AgentState):
    return Command(
        interrupt=Interrupt(
            message="Please confirm if we should continue"
        )
    )

# Next node processes human response
def process_feedback(state: AgentState):
    return {"question": f"User confirmed: {state['human_feedback']}"}

# Build the graph
workflow = StateGraph(AgentState)
workflow.add_node("ask_human", ask_human)
workflow.add_node("process_feedback", process_feedback)

workflow.set_entry_point("ask_human")
workflow.add_edge("ask_human", "process_feedback")
workflow.add_edge("process_feedback", END)

app = workflow.compile()
```

---

### **Running the Graph**

```python
config = {"configurable": {"thread_id": "456"}}

# Invoke â†’ returns Interrupt command
result = app.invoke({"question": "Should I proceed?"}, config=config)
print(result)
# Output: Interrupt(message="Please confirm if we should continue")

# Human provides response â†’ resume graph
final = app.resume("Yes, continue", config=config)
print(final)
# Output: {'question': 'User confirmed: Yes, continue', 'human_feedback': 'Yes, continue'}
```

---

### **How It Works**

* The node returns an **explicit Interrupt command** instead of pausing directly.
* LangGraph immediately stops execution.
* Your app inspects the returned `Interrupt` object and decides what to do.
* Later, `.resume()` continues execution.

âœ… **When to use:**

* When you want **more control** over interrupts.
* When working with **complex compiled graphs**.
* When you need **custom metadata** or logging around interrupts.

---

## **3. Comparison â€” Two Ways of Using Interrupt**

| **Aspect**        | **`interrupt()` Function** âœ… | **`Interrupt` Command Class** ðŸ§©       |
| ----------------- | ---------------------------- | -------------------------------------- |
| **Ease of use**   | Simple & intuitive           | More verbose                           |
| **Where used**    | Inside node functions        | Inside nodes returning commands        |
| **Return type**   | Pauses immediately           | Returns `Command(interrupt=...)`       |
| **Control level** | Automatic pause/resume       | Full control over behavior             |
| **Best for**      | Quick HITL feedback          | Complex workflows with compiled graphs |

---

## **4. Human-in-the-Loop Workflow in LangGraph**

Both methods enable HITL:

* **Agent plans â†’ Interrupts â†’ Human approves â†’ Agent resumes**
* Used in:

  * Approving AI-generated reports
  * Validating LLM outputs before executing actions
  * Safely using external APIs/tools

---

## **5. Recommendation**

* **Use `interrupt()`** â†’ when building **simple HITL** workflows âœ…
* **Use `Interrupt` Command** â†’ when you need **custom commands** or **complex compiled workflows** ðŸ§©


# Operations with Interrupts:

1. Resume -> Continue execution with input from the user without modifying the state.
2. Update and Resume -> Update the state and then continue execution.
3. Rewind/ time Travel -> Go back to a previous checkpoint in the execution
4. Branch -> Create a new branch from the current execution state to explore alternative paths.


"""

 
